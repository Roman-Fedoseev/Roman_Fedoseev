{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735ba8a8-f325-4dfb-adf5-e9fb8b5e3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Math, Latex\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8618742-674a-4436-a8c5-760caf43442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Моделирование\n",
    "# CatBoostRegressor()Для прогнозирования 5-го и 95-го квантилей использовались два параметра. Для проведения конформизации из обучающей выборки был выделен набор данных для валидации.\n",
    "\n",
    "# Эмпирическим путем я обнаружил, что вместо использования одного поправочного члена qточнее было разделить его на q_highи q_low, то есть\n",
    "\n",
    "# ### same q\n",
    "# # nonconformity = np.maximum(calib_preds[0] - y_calib, y_calib - calib_preds[1])\n",
    "# # q_low = q_high = np.quantile(nonconformity, (1 - alpha))\n",
    "\n",
    "# ### split q\n",
    "# nonconf_low = calib_preds[0] - y_calib\n",
    "# nonconf_high = y_calib - calib_preds[1]\n",
    "# q_low = np.quantile(nonconf_low, (1 - alpha / 2))\n",
    "# q_high = np.quantile(nonconf_high, (1 - alpha / 2))\n",
    "\n",
    "# for p in [train_preds, calib_preds, test_preds]:\n",
    "#     p[0] -= q_low\n",
    "#     p[1] += q_high\n",
    "# Я обнаружил, что параметры по умолчанию во всех программах xgboost, lightgbm, catboostдавали оценки, которые были недооценены. Я лишь немного поработал над выбором гиперпараметров вручную и остановился на том, что казалось \"достаточно хорошим\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3b2a2c-e04e-41b2-9f0c-dc6356bc061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinball + coverage penalty для нейросети "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536928eb-bbcc-45bf-9915-593457b3e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение моделей для расчета калибровки...\n",
      "0:\tlearn: 0.1428280\ttest: 0.1346232\tbest: 0.1346232 (0)\ttotal: 99.9ms\tremaining: 4m 59s\n",
      "100:\tlearn: 0.0860270\ttest: 0.0921969\tbest: 0.0921843 (94)\ttotal: 2.84s\tremaining: 1m 21s\n",
      "200:\tlearn: 0.0812801\ttest: 0.0923276\tbest: 0.0919774 (140)\ttotal: 5.31s\tremaining: 1m 13s\n",
      "300:\tlearn: 0.0785447\ttest: 0.0929835\tbest: 0.0919774 (140)\ttotal: 7.8s\tremaining: 1m 9s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.09197735328\n",
      "bestIteration = 140\n",
      "\n",
      "Shrink model to first 141 iterations.\n",
      "0:\tlearn: 0.0986558\ttest: 0.0928609\tbest: 0.0928609 (0)\ttotal: 24.1ms\tremaining: 1m 12s\n",
      "100:\tlearn: 0.0811582\ttest: 0.0884697\tbest: 0.0878753 (37)\ttotal: 2.39s\tremaining: 1m 8s\n",
      "200:\tlearn: 0.0765841\ttest: 0.0896853\tbest: 0.0878753 (37)\ttotal: 5.04s\tremaining: 1m 10s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.08787530913\n",
      "bestIteration = 37\n",
      "\n",
      "Shrink model to first 38 iterations.\n",
      "\n",
      "Поиск оптимального alpha на валидации...\n",
      "Лучший Alpha: 0.580\n",
      "Лучший IoU на валидации: 0.2818\n",
      "Поправки: q_low = 0.0455, q_high = 0.0288\n",
      "\n",
      "Обучение финальных моделей на полном датасете...\n",
      "0:\tlearn: 0.1409892\ttotal: 21.8ms\tremaining: 3.04s\n",
      "100:\tlearn: 0.0870379\ttotal: 2.1s\tremaining: 831ms\n",
      "140:\tlearn: 0.0854228\ttotal: 2.94s\tremaining: 0us\n",
      "0:\tlearn: 0.0976495\ttotal: 20.5ms\tremaining: 759ms\n",
      "37:\tlearn: 0.0857787\ttotal: 793ms\tremaining: 0us\n",
      "Создание предсказаний с применением лучших Q...\n",
      "Готово! Использован подобранный alpha=0.580.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Глобальная фиксация для всех библиотек\n",
    "SEED = 322\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def calculate_iou(lower_true, upper_true, lower_pred, upper_pred, epsilon=1e-6):\n",
    "    intersection = np.maximum(0, np.minimum(upper_true, upper_pred) - np.maximum(lower_true, lower_pred))\n",
    "    union = (upper_true - lower_true + epsilon) + (upper_pred - lower_pred + epsilon) - intersection\n",
    "    return np.mean(intersection / union)\n",
    "\n",
    "# 1. ЗАГРУЗКА\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train = train[train['price_p05'] > 0]\n",
    "train['dt'] = pd.to_datetime(train['dt'])\n",
    "test['dt'] = pd.to_datetime(test['dt'])\n",
    "\n",
    "# 2. FEATURE ENGINEERING\n",
    "def create_smart_features(df, train_ref=None):\n",
    "    if train_ref is not None:\n",
    "        prod_price_map = train_ref.groupby('product_id')['price_p05'].mean().to_dict()\n",
    "        df['global_prod_avg'] = df['product_id'].map(prod_price_map)\n",
    "        cat_price_map = train_ref.groupby('third_category_id')['price_p05'].mean().to_dict()\n",
    "        df['global_cat_avg'] = df['third_category_id'].map(cat_price_map)\n",
    "    \n",
    "    cat_stores_map = df.groupby('third_category_id')['n_stores'].transform('mean')\n",
    "    df['store_density_ratio'] = df['n_stores'] / (cat_stores_map + 1e-6)\n",
    "    df['temp_hum_index'] = df['avg_temperature'] * (df['avg_humidity'] / 100)\n",
    "    df['category_breadth'] = df.groupby(['dt', 'third_category_id'])['product_id'].transform('nunique')\n",
    "    return df\n",
    "\n",
    "train = create_smart_features(train, train_ref=train)\n",
    "test = create_smart_features(test, train_ref=train)\n",
    "\n",
    "# 3. ЦИКЛИЧЕСКИЕ ПРИЗНАКИ\n",
    "def add_cyclical_features(df):\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    return df\n",
    "\n",
    "train = add_cyclical_features(train)\n",
    "test = add_cyclical_features(test)\n",
    "\n",
    "# 4. СПИСОК ПРИЗНАКОВ\n",
    "cat_features = ['management_group_id', 'first_category_id', 'activity_flag']\n",
    "features = [\n",
    "    'n_stores', 'precpt', 'avg_temperature', 'avg_humidity', \n",
    "    'avg_wind_level', 'week_of_year', 'month_sin', 'month_cos',\n",
    "    'global_prod_avg', 'global_cat_avg', 'store_density_ratio',\n",
    "    'temp_hum_index', 'category_breadth'\n",
    "] + cat_features\n",
    "\n",
    "train[features] = train[features].fillna(train[features].mean(numeric_only=True))\n",
    "test[features] = test[features].fillna(train[features].mean(numeric_only=True))\n",
    "\n",
    "# 5. ВАЛИДАЦИЯ / КАЛИБРОВКА\n",
    "train_parts, val_parts = [], []\n",
    "for _, group in train.groupby('dt'):\n",
    "    group = group.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    split_idx = int(len(group) * 0.8)\n",
    "    train_parts.append(group.iloc[:split_idx])\n",
    "    val_parts.append(group.iloc[split_idx:])\n",
    "\n",
    "train_part = pd.concat(train_parts)\n",
    "val_part = pd.concat(val_parts)\n",
    "\n",
    "# ПАРАМЕТРЫ CatBoost (Optuna)\n",
    "best_optuna_params = {\n",
    "    'learning_rate': 0.09981859399773757, \n",
    "    'depth': 7, \n",
    "    'l2_leaf_reg': 8.533654459953857, \n",
    "    'random_strength': 1.3535330397123693, \n",
    "    'bagging_temperature': 0.757231412091197\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    'iterations': 3000,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 100,\n",
    "    'early_stopping_rounds': 200,\n",
    "    **best_optuna_params\n",
    "}\n",
    "\n",
    "# Обучение на частях для калибровки\n",
    "print(\"Обучение моделей для расчета калибровки...\")\n",
    "model_low = CatBoostRegressor(loss_function='MAE', **cb_params)\n",
    "model_high = CatBoostRegressor(loss_function='MAE', **cb_params)\n",
    "\n",
    "model_low.fit(train_part[features], train_part['price_p05'], \n",
    "              eval_set=(val_part[features], val_part['price_p05']), \n",
    "              cat_features=cat_features, use_best_model=True)\n",
    "\n",
    "model_high.fit(train_part[features], train_part['price_p95'], \n",
    "               eval_set=(val_part[features], val_part['price_p95']), \n",
    "               cat_features=cat_features, use_best_model=True)\n",
    "\n",
    "# --- НОВЫЙ БЛОК: ПОИСК ЛУЧШЕГО ALPHA ЧЕРЕЗ IoU ---\n",
    "print(\"\\nПоиск оптимального alpha на валидации...\")\n",
    "calib_preds_low = model_low.predict(val_part[features])\n",
    "calib_preds_high = model_high.predict(val_part[features])\n",
    "\n",
    "nonconf_low = calib_preds_low - val_part['price_p05']\n",
    "nonconf_high = val_part['price_p95'] - calib_preds_high\n",
    "\n",
    "best_alpha = 0.1\n",
    "best_iou = -1\n",
    "best_q = (0, 0)\n",
    "\n",
    "# Проверяем разные уровни уверенности\n",
    "for a in np.linspace(0.1, 0.9, 81): \n",
    "    ql = np.quantile(nonconf_low, 1 - a/2)\n",
    "    qh = np.quantile(nonconf_high, 1 - a/2)\n",
    "    \n",
    "    p05_corr = calib_preds_low - ql\n",
    "    p95_corr = calib_preds_high + qh\n",
    "    p95_corr = np.maximum(p95_corr, p05_corr + 0.001)\n",
    "    \n",
    "    current_iou = calculate_iou(\n",
    "        val_part['price_p05'].values, \n",
    "        val_part['price_p95'].values, \n",
    "        p05_corr, \n",
    "        p95_corr\n",
    "    )\n",
    "    \n",
    "    if current_iou > best_iou:\n",
    "        best_iou = current_iou\n",
    "        best_alpha = a\n",
    "        best_q = (ql, qh)\n",
    "\n",
    "q_low, q_high = best_q\n",
    "print(f\"Лучший Alpha: {best_alpha:.3f}\")\n",
    "print(f\"Лучший IoU на валидации: {best_iou:.4f}\")\n",
    "print(f\"Поправки: q_low = {q_low:.4f}, q_high = {q_high:.4f}\\n\")\n",
    "# ------------------------------------------------\n",
    "\n",
    "# 6. ФИНАЛЬНОЕ ОБУЧЕНИЕ\n",
    "print(\"Обучение финальных моделей на полном датасете...\")\n",
    "final_params_low = cb_params.copy()\n",
    "final_params_low['iterations'] = model_low.get_best_iteration() + 1\n",
    "final_params_low.pop('early_stopping_rounds', None)\n",
    "\n",
    "final_params_high = cb_params.copy()\n",
    "final_params_high['iterations'] = model_high.get_best_iteration() + 1\n",
    "final_params_high.pop('early_stopping_rounds', None)\n",
    "\n",
    "final_low = CatBoostRegressor(loss_function='MAE', allow_writing_files=False, **final_params_low)\n",
    "final_high = CatBoostRegressor(loss_function='MAE', allow_writing_files=False, **final_params_high)\n",
    "\n",
    "final_low.fit(train[features], train['price_p05'], cat_features=cat_features, verbose=100)\n",
    "final_high.fit(train[features], train['price_p95'], cat_features=cat_features, verbose=100)\n",
    "\n",
    "# 7. ПРЕДСКАЗАНИЕ С КОРРЕКТИРОВКОЙ\n",
    "print(\"Создание предсказаний с применением лучших Q...\")\n",
    "test['price_p05'] = final_low.predict(test[features]) - q_low\n",
    "test['price_p95'] = final_high.predict(test[features]) + q_high\n",
    "\n",
    "# Страховка от пересечения границ\n",
    "test['price_p95'] = np.maximum(test['price_p95'], test['price_p05'] + 0.001)\n",
    "\n",
    "submission = test[['row_id', 'price_p05', 'price_p95']].sort_values('row_id')\n",
    "submission.to_csv('submission_conformal_optimized.csv', index=False)\n",
    "print(f\"Готово! Использован подобранный alpha={best_alpha:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a46dd-a025-4790-bea4-e02c2d0d306c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f20074-ba6c-4d30-a7e5-6ea5ae1c3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# идеи - признаки через стандартные отклонения и т д в разных разрезах?\n",
    "# анализ текущих ошибок модели на валидации\n",
    "# замена валидации на OOT ?\n",
    "# обучение нейросети + conformal + кастомный лосс Pinball + coverage penalty для нейросети "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4203c79-8668-427e-b53d-56b5926f7c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134a5c7-863a-4a3d-b263-27451a9efe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
