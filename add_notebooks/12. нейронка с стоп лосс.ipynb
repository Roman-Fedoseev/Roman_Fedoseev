{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790ebb2-ab68-4cce-903b-b00e9817de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# ==========================================\n",
    "# 0. НАСТРОЙКИ И КОНФИГУРАЦИЯ\n",
    "# ==========================================\n",
    "SEED = 993\n",
    "\n",
    "NN_CONFIG = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 4096,\n",
    "    'epochs': 1000,          # Ставим с запасом, Early Stopping остановит раньше\n",
    "    'patience': 50,          # Сколько эпох ждать улучшения\n",
    "    'hidden_layers': [512, 256, 128, 64, 32], \n",
    "    'dropout': 0.2,         \n",
    "    'loss_type': 'MAE', # Рекомендую Quantile для этой задачи\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# ==========================================\n",
    "# 1. ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ\n",
    "# ==========================================\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train = train[train['price_p05'] > 0]\n",
    "train['dt'] = pd.to_datetime(train['dt'])\n",
    "test['dt'] = pd.to_datetime(test['dt'])\n",
    "\n",
    "# 2. FEATURE ENGINEERING\n",
    "def create_smart_features(df, train_ref=None):\n",
    "    if train_ref is not None:\n",
    "        prod_price_map = train_ref.groupby('product_id')['price_p05'].mean().to_dict()\n",
    "        df['global_prod_avg'] = df['product_id'].map(prod_price_map)\n",
    "        cat_price_map = train_ref.groupby('third_category_id')['price_p05'].mean().to_dict()\n",
    "        df['global_cat_avg'] = df['third_category_id'].map(cat_price_map)\n",
    "    \n",
    "    cat_stores_map = df.groupby('third_category_id')['n_stores'].transform('mean')\n",
    "    df['store_density_ratio'] = df['n_stores'] / (cat_stores_map + 1e-6)\n",
    "    df['temp_hum_index'] = df['avg_temperature'] * (df['avg_humidity'] / 100)\n",
    "    df['category_breadth'] = df.groupby(['dt', 'third_category_id'])['product_id'].transform('nunique')\n",
    "    return df\n",
    "\n",
    "train = create_smart_features(train, train_ref=train)\n",
    "test = create_smart_features(test, train_ref=train)\n",
    "\n",
    "def add_cyclical_features(df):\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    return df\n",
    "\n",
    "train = add_cyclical_features(train)\n",
    "test = add_cyclical_features(test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. ПОДГОТОВКА К НЕЙРОСЕТИ\n",
    "# ==========================================\n",
    "cat_cols = ['management_group_id', 'first_category_id', 'activity_flag', 'product_id', 'third_category_id']\n",
    "num_cols = [\n",
    "    'n_stores', 'precpt', 'avg_temperature', 'avg_humidity', \n",
    "    'avg_wind_level', 'week_of_year', 'month_sin', 'month_cos',\n",
    "    'global_prod_avg', 'global_cat_avg', 'store_density_ratio',\n",
    "    'temp_hum_index', 'category_breadth'\n",
    "]\n",
    "\n",
    "# 3.1 Заполнение пропусков\n",
    "train[num_cols] = train[num_cols].fillna(train[num_cols].mean())\n",
    "test[num_cols] = test[num_cols].fillna(train[num_cols].mean())\n",
    "train[cat_cols] = train[cat_cols].fillna(-1)\n",
    "test[cat_cols] = test[cat_cols].fillna(-1)\n",
    "\n",
    "# 3.2 Label Encoding \n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    full_col = pd.concat([train[col].astype(str), test[col].astype(str)], axis=0)\n",
    "    le.fit(full_col)\n",
    "    train[col] = le.transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 3.3 Scaling \n",
    "scaler = StandardScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "\n",
    "# 3.4 Правило размера эмбеддингов\n",
    "embedding_sizes = []\n",
    "for col in cat_cols:\n",
    "    num_unique = len(label_encoders[col].classes_)\n",
    "    emb_dim = min(50, (num_unique + 1) // 2)\n",
    "    embedding_sizes.append((num_unique, emb_dim))\n",
    "    print(f\"Feature '{col}': {num_unique} unique -> embedding size {emb_dim}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. МОДЕЛЬ НЕЙРОСЕТИ И УТИЛИТЫ\n",
    "# ==========================================\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target=None):\n",
    "        self.cats = df[cat_cols].values.astype(np.int64)\n",
    "        self.nums = df[num_cols].values.astype(np.float32)\n",
    "        self.target = df[target].values.astype(np.float32) if target is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.target is not None:\n",
    "            return self.cats[idx], self.nums[idx], self.target[idx]\n",
    "        return self.cats[idx], self.nums[idx]\n",
    "\n",
    "class TabularNN(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont, hidden_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_cont = n_cont\n",
    "        \n",
    "        layers = []\n",
    "        in_size = self.n_emb + self.n_cont\n",
    "        \n",
    "        for h_size in hidden_layers:\n",
    "            layers.append(nn.Linear(in_size, h_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(h_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_size = h_size\n",
    "            \n",
    "        layers.append(nn.Linear(in_size, 1)) \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        return self.layers(x).squeeze(1)\n",
    "\n",
    "# КЛАСС EARLY STOPPING\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "\n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile):\n",
    "        super().__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        errors = target - preds\n",
    "        loss = torch.max((self.quantile - 1) * errors, self.quantile * errors)\n",
    "        return torch.abs(loss).mean()\n",
    "\n",
    "def get_loss_fn(name, quantile=None):\n",
    "    if name == 'MSE': return nn.MSELoss()\n",
    "    if name == 'MAE': return nn.L1Loss()\n",
    "    if name == 'Quantile': return QuantileLoss(quantile)\n",
    "    raise ValueError(\"Unknown Loss Type\")\n",
    "\n",
    "# Функция обучения одной эпохи\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for cats, nums, targets in loader:\n",
    "        cats, nums, targets = cats.to(device), nums.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(cats, nums)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Функция расчета лосса на валидации (для Early Stopping)\n",
    "def validate_loss(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 3:\n",
    "                cats, nums, targets = batch\n",
    "            else:\n",
    "                continue # Если нет таргета, нельзя посчитать лосс\n",
    "            \n",
    "            cats, nums, targets = cats.to(device), nums.to(device), targets.to(device)\n",
    "            preds = model(cats, nums)\n",
    "            loss = criterion(preds, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Функция предсказания\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 3:\n",
    "                cats, nums, _ = batch\n",
    "            else:\n",
    "                cats, nums = batch\n",
    "            \n",
    "            cats, nums = cats.to(device), nums.to(device)\n",
    "            preds = model(cats, nums)\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "    return np.concatenate(preds_list)\n",
    "\n",
    "# ==========================================\n",
    "# 5. ОБУЧЕНИЕ И ВАЛИДАЦИЯ\n",
    "# ==========================================\n",
    "train_parts, val_parts = [], []\n",
    "for _, group in train.groupby('dt'):\n",
    "    group = group.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    split_idx = int(len(group) * 0.8)\n",
    "    train_parts.append(group.iloc[:split_idx])\n",
    "    val_parts.append(group.iloc[split_idx:])\n",
    "\n",
    "train_part = pd.concat(train_parts)\n",
    "val_part = pd.concat(val_parts)\n",
    "\n",
    "# Датасеты\n",
    "train_ds_low = TabularDataset(train_part, cat_cols, num_cols, 'price_p05')\n",
    "val_ds_low = TabularDataset(val_part, cat_cols, num_cols, 'price_p05')\n",
    "train_ds_high = TabularDataset(train_part, cat_cols, num_cols, 'price_p95')\n",
    "val_ds_high = TabularDataset(val_part, cat_cols, num_cols, 'price_p95')\n",
    "\n",
    "loaders = {\n",
    "    'train_low': DataLoader(train_ds_low, batch_size=NN_CONFIG['batch_size'], shuffle=True),\n",
    "    'val_low': DataLoader(val_ds_low, batch_size=NN_CONFIG['batch_size']*2),\n",
    "    'train_high': DataLoader(train_ds_high, batch_size=NN_CONFIG['batch_size'], shuffle=True),\n",
    "    'val_high': DataLoader(val_ds_high, batch_size=NN_CONFIG['batch_size']*2)\n",
    "}\n",
    "\n",
    "device = NN_CONFIG['device']\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train_and_validate_with_es(target_name, quantile, train_loader, val_loader):\n",
    "    print(f\"\\n=== Training Model for {target_name} (Loss: {NN_CONFIG['loss_type']}) ===\")\n",
    "    \n",
    "    model = TabularNN(embedding_sizes, len(num_cols), NN_CONFIG['hidden_layers'], NN_CONFIG['dropout']).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=NN_CONFIG['learning_rate'])\n",
    "    criterion = get_loss_fn(NN_CONFIG['loss_type'], quantile)\n",
    "    \n",
    "    # Инициализация Early Stopping\n",
    "    early_stopper = EarlyStopping(patience=NN_CONFIG['patience'])\n",
    "    \n",
    "    for epoch in range(NN_CONFIG['epochs']):\n",
    "        # Шаг обучения\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Шаг валидации\n",
    "        val_loss = validate_loss(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Проверка Early Stopping\n",
    "        early_stopper(val_loss, model)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or early_stopper.early_stop:\n",
    "            print(f\"Epoch {epoch+1}/{NN_CONFIG['epochs']} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f}\")\n",
    "        \n",
    "        if early_stopper.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "            \n",
    "    # Загружаем лучшие веса\n",
    "    print(\"Loading best model weights...\")\n",
    "    model.load_state_dict(early_stopper.best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Обучаем валидационные модели\n",
    "model_low_val = train_and_validate_with_es('price_p05', 0.05, loaders['train_low'], loaders['val_low'])\n",
    "model_high_val = train_and_validate_with_es('price_p95', 0.95, loaders['train_high'], loaders['val_high'])\n",
    "\n",
    "# РАСЧЕТ МЕТРИКИ (IoU)\n",
    "def calculate_iou(lower_true, upper_true, lower_pred, upper_pred, epsilon=1e-6):\n",
    "    intersection = np.maximum(0, np.minimum(upper_true, upper_pred) - np.maximum(lower_true, lower_pred))\n",
    "    union = (upper_true - lower_true + epsilon) + (upper_pred - lower_pred + epsilon) - intersection\n",
    "    return np.mean(intersection / union)\n",
    "\n",
    "print(\"Calculating Validation Metrics...\")\n",
    "preds_low = predict(model_low_val, loaders['val_low'], device)\n",
    "preds_high = predict(model_high_val, loaders['val_high'], device)\n",
    "val_part['pred_p05'] = preds_low\n",
    "val_part['pred_p95'] = np.maximum(preds_high, preds_low + 0.001)\n",
    "\n",
    "iou_score = calculate_iou(\n",
    "    val_part['price_p05'], val_part['price_p95'],\n",
    "    val_part['pred_p05'], val_part['pred_p95']\n",
    ")\n",
    "print(f\"\\n>>> VALIDATION IoU SCORE: {iou_score:.5f} <<<\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ==========================================\n",
    "# 6. ФИНАЛЬНОЕ ОБУЧЕНИЕ (FULL TRAIN)\n",
    "# ==========================================\n",
    "print(\"Retraining on FULL dataset...\")\n",
    "full_ds_low = TabularDataset(train, cat_cols, num_cols, 'price_p05')\n",
    "full_ds_high = TabularDataset(train, cat_cols, num_cols, 'price_p95')\n",
    "\n",
    "full_loader_low = DataLoader(full_ds_low, batch_size=NN_CONFIG['batch_size'], shuffle=True)\n",
    "full_loader_high = DataLoader(full_ds_high, batch_size=NN_CONFIG['batch_size'], shuffle=True)\n",
    "\n",
    "# ХАК: Для финального обучения используем train выборку как валидационную\n",
    "# Это позволяет остановить обучение, когда модель перестанет улучшаться (сходимость)\n",
    "# и выбрать лучшую точку сходимости.\n",
    "final_model_low = train_and_validate_with_es('Final Low', 0.05, full_loader_low, full_loader_low)\n",
    "final_model_high = train_and_validate_with_es('Final High', 0.95, full_loader_high, full_loader_high)\n",
    "\n",
    "# ==========================================\n",
    "# 7. ПРЕДСКАЗАНИЕ\n",
    "# ==========================================\n",
    "print(\"Generating submission...\")\n",
    "test_ds = TabularDataset(test, cat_cols, num_cols, None)\n",
    "test_loader = DataLoader(test_ds, batch_size=NN_CONFIG['batch_size']*2, shuffle=False)\n",
    "\n",
    "test['price_p05'] = predict(final_model_low, test_loader, device)\n",
    "test['price_p95'] = predict(final_model_high, test_loader, device)\n",
    "\n",
    "test['price_p95'] = np.maximum(test['price_p95'], test['price_p05'] + 0.001)\n",
    "\n",
    "submission = test[['row_id', 'price_p05', 'price_p95']].sort_values('row_id')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Готово! Результаты зафиксированы с seed {SEED}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14acff7d-72ef-4c36-ae83-a751df7ed575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94992e-2415-4b9c-9300-a2856dd5b608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c11b1-7fed-426b-931c-bf49c8015816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
